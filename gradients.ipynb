{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-02T20:06:24.517789Z",
     "start_time": "2024-10-02T20:06:23.312851Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "from Data.loader_from_array import MedMNIST_Testing\n",
    "from Train.train_one_epoch import eval_epoch_save"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:06:26.270149Z",
     "start_time": "2024-10-02T20:06:26.138331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = models.densenet121()\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 2))"
   ],
   "id": "b223fca82afa7b44",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:06:27.885306Z",
     "start_time": "2024-10-02T20:06:27.881929Z"
    }
   },
   "cell_type": "code",
   "source": "names = ['features.denseblock1.', 'features.denseblock2.', 'features.denseblock3.', 'features.denseblock4.', 'classifier.']",
   "id": "4bbf7600899b0caf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:06:31.782364Z",
     "start_time": "2024-10-02T20:06:30.503364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loaded = torch.load('/home/zoe/GhassanGT Dropbox/Zoe Fowler/Zoe/InSync/PhDResearch/Code/Federated-SuperResolution/Saved/pneumonia/classification/model.pth')\n",
    "model.load_state_dict(loaded)"
   ],
   "id": "8b23426c4f2bcab0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:06:34.392746Z",
     "start_time": "2024-10-02T20:06:34.366532Z"
    }
   },
   "cell_type": "code",
   "source": "model = model.cuda()\n",
   "id": "d29fb5de6a0a3487",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T21:26:55.461094Z",
     "start_time": "2024-10-02T21:26:55.455559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_gradients(data_loader, model, criterion, device='cuda'):\n",
    "    \n",
    "    total_grads = []\n",
    "        \n",
    "    for i, (image, target) in enumerate(data_loader):\n",
    "        \n",
    "        layer_grads = {}\n",
    "        for pref in names:\n",
    "            layer_grads[pref] = []\n",
    "        #layer_grads = []\n",
    "            \n",
    "        one_hot = torch.zeros(target.shape[0], 2)\n",
    "        one_hot[range(target.shape[0]), target.long()] = 1\n",
    "        input = image.to(device)\n",
    "        one_hot = one_hot.to(device)\n",
    "        input = input.float()\n",
    "        model.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = criterion(output, one_hot)\n",
    "        # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "        loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in names:\n",
    "                if param.grad is not None:\n",
    "                    #layer_grads.append(np.mean(np.linalg.norm(param.grad.detach().cpu().numpy())))\n",
    "                    layer_grads[pref].append(np.linalg.norm(param.grad.detach().cpu().numpy()))\n",
    "\n",
    "        # for pref in names:\n",
    "        #     if len(layer_grads[pref]) > 0:\n",
    "        #         layer_grads[pref] = np.mean(layer_grads[pref])\n",
    "        if i == 0 :\n",
    "            print(layer_grads)\n",
    "        #total_grads.append(np.mean(np.array(layer_grads)))\n",
    "\n",
    "    return total_grads"
   ],
   "id": "cd2636595b3e8724",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T21:27:14.407914Z",
     "start_time": "2024-10-02T21:27:08.953224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Models.SRResNet import SRResNet\n",
    "from torch.utils.data import DataLoader\n",
    "from Data.medmnist_loader import MedMNISTDataset\n",
    "from Train.train_one_epoch import eval_epoch_save\n",
    "\n",
    "# Try scale x 4 on model for x 8 and x 4\n",
    "# load data and create test loader\n",
    "first = 56\n",
    "scale_first = int(224 / 56)\n",
    "test_dataset1 = MedMNISTDataset(mode='test', low_size=first)\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=1, shuffle=False)\n",
    "# establish models\n",
    "model1 = SRResNet(scaling_factor=4)\n",
    "model2 = SRResNet(scaling_factor=4)\n",
    "device = 'cuda'\n",
    "wts1 = torch.load('/home/zoe/GhassanGT Dropbox/Zoe Fowler/Zoe/InSync/PhDResearch/Code/Federated-SuperResolution/Saved/pneumonia/best_model_4_resnet.pth')\n",
    "wts2 = torch.load('/home/zoe/GhassanGT Dropbox/Zoe Fowler/Zoe/InSync/PhDResearch/Code/Federated-SuperResolution/Saved/pneumonia/best_model_8_resnet.pth')\n",
    "model1.load_state_dict(wts1, strict=False)\n",
    "model2.load_state_dict(wts2, strict=False)\n",
    "model1 = model1.to(device)\n",
    "model2 = model2.to(device)\n",
    "criterion = nn.L1Loss().to(device)\n",
    "\n",
    "print('Evaluating in distribution')\n",
    "_, acc, outputs_indistrib = eval_epoch_save(data_loader=test_loader1, model=model1, criterion=criterion, device=device)\n",
    "print('test psnr: ', acc)\n",
    "# print('Evaluating out of distribution')\n",
    "# _, acc, outputs_outdistrib = eval_epoch_save(data_loader=test_loader1, model=model2, criterion=criterion, device=device)\n",
    "# print('test psnr: ', acc)"
   ],
   "id": "a23685d9ed182bf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /media/zoe/ssd/pneumoniamnist_224.npz\n",
      "Evaluating in distribution\n",
      "test psnr:  tensor(34.3951, device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T21:27:35.528023480Z",
     "start_time": "2024-10-02T21:27:18.180163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Data.loader_from_array import MedMNIST_Testing\n",
    "# form new dataset and loader of outputs\n",
    "new_dataset1 = MedMNIST_Testing(input_arr = outputs_indistrib)\n",
    "new_loader_test1 = DataLoader(new_dataset1, batch_size=1, shuffle=False)\n",
    "ll = get_gradients(data_loader=new_loader_test1, model=model, criterion=criterion, device=device)"
   ],
   "id": "79b695ad891d7d47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features.denseblock1': [0.2319102, 0.05846045, 1.4050945, 0.19895133, 0.09362672, 5.016035, 0.2932584, 0.09562707, 2.1293051, 0.2129423, 0.15873975, 5.649084, 0.2528194, 0.114099935, 2.0174346, 0.2111632, 0.14060876, 5.3234015, 0.21310095, 0.11778922, 1.8923578, 0.17163879, 0.12833926, 4.529271, 0.18748188, 0.1274887, 1.9307299, 0.19346951, 0.15858296, 4.092855, 0.14934824, 0.08542499, 1.5332464, 0.11596509, 0.12112134, 3.307009], 'features.denseblock2': [0.21627392, 0.1918361, 1.9442198, 0.21438509, 0.17882052, 5.296839, 0.24150787, 0.16844031, 2.1862109, 0.2038738, 0.15605606, 5.031063, 0.18203834, 0.14650594, 2.042695, 0.19059645, 0.15926647, 4.2690997, 0.16261187, 0.13049312, 1.870927, 0.1537934, 0.1280824, 3.479949, 0.13738832, 0.11322036, 1.6889151, 0.11090076, 0.106667675, 2.8229852, 0.11785458, 0.09718376, 1.3761982, 0.09315555, 0.09151158, 2.241416, 0.11595513, 0.10348609, 1.4749779, 0.09988447, 0.09447622, 2.401064, 0.097224005, 0.078973286, 1.3030804, 0.083657995, 0.08237666, 1.9163231, 0.09518067, 0.07956248, 1.3189679, 0.074010454, 0.07347801, 1.7797374, 0.08607176, 0.073104754, 1.2367061, 0.07730063, 0.07279583, 1.9004399, 0.07610952, 0.07394259, 1.1201526, 0.06201441, 0.06955867, 1.5341234, 0.078574434, 0.07494951, 1.2284418, 0.06835157, 0.06389614, 1.5844752], 'features.denseblock3': [0.094714664, 0.07661633, 1.0627332, 0.07285254, 0.06684539, 1.8401991, 0.07004845, 0.06761412, 0.8705739, 0.05562156, 0.05662572, 1.3186698, 0.07198899, 0.061987203, 0.8689633, 0.047467563, 0.05473148, 1.2328377, 0.059240907, 0.051003594, 0.774864, 0.053569786, 0.047644142, 1.296824, 0.05581845, 0.046449568, 0.7779021, 0.04897272, 0.04423917, 1.0521334, 0.04837445, 0.042061143, 0.72203475, 0.031306434, 0.032026026, 0.8785738, 0.049700506, 0.042303495, 0.7171211, 0.035565358, 0.03461272, 0.969838, 0.035384186, 0.030403325, 0.54336673, 0.03251621, 0.023290243, 0.71632177, 0.04319887, 0.037492424, 0.6458812, 0.030756377, 0.030268757, 0.7814634, 0.035706986, 0.031191666, 0.5900585, 0.029423792, 0.02736558, 0.7052386, 0.034722477, 0.031622615, 0.5651664, 0.028168786, 0.029717404, 0.6602971, 0.031287882, 0.027759822, 0.55463576, 0.02426841, 0.023702566, 0.6249089, 0.02861444, 0.024719007, 0.53298366, 0.025090292, 0.02272471, 0.58437175, 0.028858868, 0.026380619, 0.52993214, 0.027183598, 0.0259935, 0.60894984, 0.029560177, 0.026264243, 0.54108596, 0.022185478, 0.023816925, 0.5617892, 0.026052896, 0.023274424, 0.49449933, 0.01981864, 0.020057563, 0.52688515, 0.026830653, 0.024765888, 0.5428401, 0.021601444, 0.020665692, 0.52571243, 0.027354773, 0.024080247, 0.54920983, 0.022106158, 0.022386616, 0.53821456, 0.022043664, 0.020413987, 0.45721245, 0.018317273, 0.01721296, 0.43704152, 0.01952249, 0.017153913, 0.4173948, 0.015855731, 0.016126618, 0.40541643, 0.021401659, 0.018426673, 0.43681657, 0.017055947, 0.016049257, 0.42915392, 0.02415445, 0.020592801, 0.5072906, 0.017258687, 0.017089566, 0.46391818, 0.018809723, 0.015450364, 0.4040335, 0.016822778, 0.0154174315, 0.39118537, 0.019823007, 0.016398825, 0.4480717, 0.014876041, 0.014602125, 0.40711987], 'features.denseblock4': [0.023386428, 0.021279402, 0.38884208, 0.016442332, 0.016162701, 0.4494521, 0.02172587, 0.019840265, 0.37408766, 0.014779141, 0.013431426, 0.41556415, 0.018889409, 0.016889207, 0.32778215, 0.017530924, 0.01711986, 0.41538367, 0.018758094, 0.016951492, 0.33810663, 0.014778191, 0.014461124, 0.38795996, 0.018324362, 0.015903195, 0.3283343, 0.015329344, 0.013493351, 0.33835456, 0.016715119, 0.014501228, 0.30197835, 0.013726944, 0.011464996, 0.31205434, 0.014806131, 0.012933557, 0.2880524, 0.012343466, 0.011927226, 0.30917004, 0.012993445, 0.01169769, 0.2601652, 0.010400086, 0.010197011, 0.28612605, 0.014288077, 0.012286537, 0.28420612, 0.012047865, 0.010873332, 0.2753909, 0.012499256, 0.011234597, 0.2589228, 0.010097288, 0.009142241, 0.2565202, 0.012893501, 0.01143956, 0.26836652, 0.012368362, 0.010237424, 0.2660852, 0.012197592, 0.010488714, 0.24553098, 0.009494833, 0.009267901, 0.23576643, 0.009662284, 0.008551966, 0.20654716, 0.007193433, 0.0071497797, 0.19944131, 0.012976797, 0.011637971, 0.28557882, 0.010087736, 0.0092893, 0.25910854, 0.010306328, 0.00903411, 0.22160232, 0.00851164, 0.0076378295, 0.20326532, 0.012060484, 0.01025937, 0.2681743, 0.010242668, 0.008853085, 0.24362938], 'classifier': [9.166992, 0.70710677]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m new_dataset1 \u001B[38;5;241m=\u001B[39m MedMNIST_Testing(input_arr \u001B[38;5;241m=\u001B[39m outputs_indistrib)\n\u001B[1;32m      4\u001B[0m new_loader_test1 \u001B[38;5;241m=\u001B[39m DataLoader(new_dataset1, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m----> 5\u001B[0m ll \u001B[38;5;241m=\u001B[39m get_gradients(data_loader\u001B[38;5;241m=\u001B[39mnew_loader_test1, model\u001B[38;5;241m=\u001B[39mmodel, criterion\u001B[38;5;241m=\u001B[39mcriterion, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "Cell \u001B[0;32mIn[22], line 8\u001B[0m, in \u001B[0;36mget_gradients\u001B[0;34m(data_loader, model, criterion, device)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_gradients\u001B[39m(data_loader, model, criterion, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m      6\u001B[0m     total_grads \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (image, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_loader):\n\u001B[1;32m     10\u001B[0m         layer_grads \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m pref \u001B[38;5;129;01min\u001B[39;00m names:\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/GhassanGT Dropbox/Zoe Fowler/Zoe/InSync/PhDResearch/Code/Federated-SuperResolution/Data/loader_from_array.py:32\u001B[0m, in \u001B[0;36mMedMNIST_Testing.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     29\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[0;32m---> 32\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n\u001B[1;32m     34\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargets[index]\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img, y\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[0;34m(self, pic)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[1;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mto_tensor(pic)\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torchvision/transforms/functional.py:172\u001B[0m, in \u001B[0;36mto_tensor\u001B[0;34m(pic)\u001B[0m\n\u001B[1;32m    170\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mview(pic\u001B[38;5;241m.\u001B[39msize[\u001B[38;5;241m1\u001B[39m], pic\u001B[38;5;241m.\u001B[39msize[\u001B[38;5;241m0\u001B[39m], F_pil\u001B[38;5;241m.\u001B[39mget_image_num_channels(pic))\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# put it from HWC to CHW format\u001B[39;00m\n\u001B[0;32m--> 172\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mByteTensor):\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\u001B[38;5;241m.\u001B[39mto(dtype\u001B[38;5;241m=\u001B[39mdefault_float_dtype)\u001B[38;5;241m.\u001B[39mdiv(\u001B[38;5;241m255\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T21:02:52.860838Z",
     "start_time": "2024-10-02T21:02:52.856297Z"
    }
   },
   "cell_type": "code",
   "source": "np.mean(np.array(ll))",
   "id": "d07f58fc3840bb19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44923592"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T21:27:47.653872351Z",
     "start_time": "2024-10-02T21:27:40.060977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Evaluating out of distribution')\n",
    "_, acc, outputs_outdistrib = eval_epoch_save(data_loader=test_loader1, model=model2, criterion=criterion, device=device)\n",
    "print('test psnr: ', acc)\n",
    "new_dataset2 = MedMNIST_Testing(input_arr = outputs_outdistrib)\n",
    "new_loader_test2 = DataLoader(new_dataset2, batch_size=1, shuffle=False)\n",
    "ll2 = get_gradients(data_loader=new_loader_test2, model=model, criterion=criterion, device=device)"
   ],
   "id": "b5876dff89851007",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating out of distribution\n",
      "test psnr:  tensor(7.0845, device='cuda:0')\n",
      "{'features.denseblock1': [0.2319102, 0.05846045, 1.4050945, 0.19895133, 0.09362672, 5.016035, 0.2932584, 0.09562707, 2.1293051, 0.2129423, 0.15873975, 5.649084, 0.2528194, 0.114099935, 2.0174346, 0.2111632, 0.14060876, 5.3234015, 0.21310095, 0.11778922, 1.8923578, 0.17163879, 0.12833926, 4.529271, 0.18748188, 0.1274887, 1.9307299, 0.19346951, 0.15858296, 4.092855, 0.14934824, 0.08542499, 1.5332464, 0.11596509, 0.12112134, 3.307009], 'features.denseblock2': [0.21627392, 0.1918361, 1.9442198, 0.21438509, 0.17882052, 5.296839, 0.24150787, 0.16844031, 2.1862109, 0.2038738, 0.15605606, 5.031063, 0.18203834, 0.14650594, 2.042695, 0.19059645, 0.15926647, 4.2690997, 0.16261187, 0.13049312, 1.870927, 0.1537934, 0.1280824, 3.479949, 0.13738832, 0.11322036, 1.6889151, 0.11090076, 0.106667675, 2.8229852, 0.11785458, 0.09718376, 1.3761982, 0.09315555, 0.09151158, 2.241416, 0.11595513, 0.10348609, 1.4749779, 0.09988447, 0.09447622, 2.401064, 0.097224005, 0.078973286, 1.3030804, 0.083657995, 0.08237666, 1.9163231, 0.09518067, 0.07956248, 1.3189679, 0.074010454, 0.07347801, 1.7797374, 0.08607176, 0.073104754, 1.2367061, 0.07730063, 0.07279583, 1.9004399, 0.07610952, 0.07394259, 1.1201526, 0.06201441, 0.06955867, 1.5341234, 0.078574434, 0.07494951, 1.2284418, 0.06835157, 0.06389614, 1.5844752], 'features.denseblock3': [0.094714664, 0.07661633, 1.0627332, 0.07285254, 0.06684539, 1.8401991, 0.07004845, 0.06761412, 0.8705739, 0.05562156, 0.05662572, 1.3186698, 0.07198899, 0.061987203, 0.8689633, 0.047467563, 0.05473148, 1.2328377, 0.059240907, 0.051003594, 0.774864, 0.053569786, 0.047644142, 1.296824, 0.05581845, 0.046449568, 0.7779021, 0.04897272, 0.04423917, 1.0521334, 0.04837445, 0.042061143, 0.72203475, 0.031306434, 0.032026026, 0.8785738, 0.049700506, 0.042303495, 0.7171211, 0.035565358, 0.03461272, 0.969838, 0.035384186, 0.030403325, 0.54336673, 0.03251621, 0.023290243, 0.71632177, 0.04319887, 0.037492424, 0.6458812, 0.030756377, 0.030268757, 0.7814634, 0.035706986, 0.031191666, 0.5900585, 0.029423792, 0.02736558, 0.7052386, 0.034722477, 0.031622615, 0.5651664, 0.028168786, 0.029717404, 0.6602971, 0.031287882, 0.027759822, 0.55463576, 0.02426841, 0.023702566, 0.6249089, 0.02861444, 0.024719007, 0.53298366, 0.025090292, 0.02272471, 0.58437175, 0.028858868, 0.026380619, 0.52993214, 0.027183598, 0.0259935, 0.60894984, 0.029560177, 0.026264243, 0.54108596, 0.022185478, 0.023816925, 0.5617892, 0.026052896, 0.023274424, 0.49449933, 0.01981864, 0.020057563, 0.52688515, 0.026830653, 0.024765888, 0.5428401, 0.021601444, 0.020665692, 0.52571243, 0.027354773, 0.024080247, 0.54920983, 0.022106158, 0.022386616, 0.53821456, 0.022043664, 0.020413987, 0.45721245, 0.018317273, 0.01721296, 0.43704152, 0.01952249, 0.017153913, 0.4173948, 0.015855731, 0.016126618, 0.40541643, 0.021401659, 0.018426673, 0.43681657, 0.017055947, 0.016049257, 0.42915392, 0.02415445, 0.020592801, 0.5072906, 0.017258687, 0.017089566, 0.46391818, 0.018809723, 0.015450364, 0.4040335, 0.016822778, 0.0154174315, 0.39118537, 0.019823007, 0.016398825, 0.4480717, 0.014876041, 0.014602125, 0.40711987], 'features.denseblock4': [0.023386428, 0.021279402, 0.38884208, 0.016442332, 0.016162701, 0.4494521, 0.02172587, 0.019840265, 0.37408766, 0.014779141, 0.013431426, 0.41556415, 0.018889409, 0.016889207, 0.32778215, 0.017530924, 0.01711986, 0.41538367, 0.018758094, 0.016951492, 0.33810663, 0.014778191, 0.014461124, 0.38795996, 0.018324362, 0.015903195, 0.3283343, 0.015329344, 0.013493351, 0.33835456, 0.016715119, 0.014501228, 0.30197835, 0.013726944, 0.011464996, 0.31205434, 0.014806131, 0.012933557, 0.2880524, 0.012343466, 0.011927226, 0.30917004, 0.012993445, 0.01169769, 0.2601652, 0.010400086, 0.010197011, 0.28612605, 0.014288077, 0.012286537, 0.28420612, 0.012047865, 0.010873332, 0.2753909, 0.012499256, 0.011234597, 0.2589228, 0.010097288, 0.009142241, 0.2565202, 0.012893501, 0.01143956, 0.26836652, 0.012368362, 0.010237424, 0.2660852, 0.012197592, 0.010488714, 0.24553098, 0.009494833, 0.009267901, 0.23576643, 0.009662284, 0.008551966, 0.20654716, 0.007193433, 0.0071497797, 0.19944131, 0.012976797, 0.011637971, 0.28557882, 0.010087736, 0.0092893, 0.25910854, 0.010306328, 0.00903411, 0.22160232, 0.00851164, 0.0076378295, 0.20326532, 0.012060484, 0.01025937, 0.2681743, 0.010242668, 0.008853085, 0.24362938], 'classifier': [9.166992, 0.70710677]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m new_dataset2 \u001B[38;5;241m=\u001B[39m MedMNIST_Testing(input_arr \u001B[38;5;241m=\u001B[39m outputs_outdistrib)\n\u001B[1;32m      5\u001B[0m new_loader_test2 \u001B[38;5;241m=\u001B[39m DataLoader(new_dataset2, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m----> 6\u001B[0m ll2 \u001B[38;5;241m=\u001B[39m get_gradients(data_loader\u001B[38;5;241m=\u001B[39mnew_loader_test2, model\u001B[38;5;241m=\u001B[39mmodel, criterion\u001B[38;5;241m=\u001B[39mcriterion, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "Cell \u001B[0;32mIn[22], line 21\u001B[0m, in \u001B[0;36mget_gradients\u001B[0;34m(data_loader, model, criterion, device)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 21\u001B[0m output \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, one_hot)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# backward pass (compute gradients of parameters w.r.t. loss)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torchvision/models/densenet.py:213\u001B[0m, in \u001B[0;36mDenseNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 213\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures(x)\n\u001B[1;32m    214\u001B[0m     out \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(features, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    215\u001B[0m     out \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39madaptive_avg_pool2d(out, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torchvision/models/densenet.py:122\u001B[0m, in \u001B[0;36m_DenseBlock.forward\u001B[0;34m(self, init_features)\u001B[0m\n\u001B[1;32m    120\u001B[0m features \u001B[38;5;241m=\u001B[39m [init_features]\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 122\u001B[0m     new_features \u001B[38;5;241m=\u001B[39m layer(features)\n\u001B[1;32m    123\u001B[0m     features\u001B[38;5;241m.\u001B[39mappend(new_features)\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(features, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torchvision/models/densenet.py:88\u001B[0m, in \u001B[0;36m_DenseLayer.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     86\u001B[0m     bottleneck_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_checkpoint_bottleneck(prev_features)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 88\u001B[0m     bottleneck_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn_function(prev_features)\n\u001B[1;32m     90\u001B[0m new_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(bottleneck_output)))\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_rate \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torchvision/models/densenet.py:49\u001B[0m, in \u001B[0;36m_DenseLayer.bn_function\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbn_function\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs: List[Tensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m     48\u001B[0m     concated_features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(inputs, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 49\u001B[0m     bottleneck_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(concated_features)))  \u001B[38;5;66;03m# noqa: T484\u001B[39;00m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bottleneck_output\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/coding/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:151\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrack_running_stats:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001B[39;00m\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_batches_tracked \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# type: ignore[has-type]\u001B[39;00m\n\u001B[0;32m--> 151\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_batches_tracked\u001B[38;5;241m.\u001B[39madd_(\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# type: ignore[has-type]\u001B[39;00m\n\u001B[1;32m    152\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmomentum \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# use cumulative moving average\u001B[39;00m\n\u001B[1;32m    153\u001B[0m             exponential_average_factor \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_batches_tracked)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T21:09:17.111815Z",
     "start_time": "2024-10-02T21:09:17.104182Z"
    }
   },
   "cell_type": "code",
   "source": "ll2",
   "id": "1e135bb1d22e70ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44439992,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.43440622,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.4711076,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.46172303,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.46536258,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.45680946,\n",
       " 0.44923592,\n",
       " 0.46337184,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44345886,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.4668208,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44266993,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592,\n",
       " 0.44923592]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
